{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e34ec889",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/polia/Venvs/ner/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-28 20:57:31.574767: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-28 20:57:31.601852: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745884651.626764   19184 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745884651.633246   19184 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1745884651.654717   19184 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745884651.654771   19184 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745884651.654774   19184 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1745884651.654776   19184 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-28 20:57:31.663356: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel, AutoModelForTokenClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fe46e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9fd5e7",
   "metadata": {},
   "source": [
    "- Prever uma palavra na máscara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61883415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.14287830889225006,\n",
       "  'token': 5028,\n",
       "  'token_str': 'pedra',\n",
       "  'sequence': 'Tinha uma pedra no meio do caminho.'},\n",
       " {'score': 0.06213388964533806,\n",
       "  'token': 7411,\n",
       "  'token_str': 'árvore',\n",
       "  'sequence': 'Tinha uma árvore no meio do caminho.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n",
    "\n",
    "pipe('Tinha uma [MASK] no meio do caminho.', top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911ed799",
   "metadata": {},
   "source": [
    "- Entidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09ef5c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "/home/polia/Venvs/ner/lib/python3.12/site-packages/transformers/pipelines/token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"AggregationStrategy.SIMPLE\"` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.9981694,\n",
       "  'word': 'Sylvain',\n",
       "  'start': 11,\n",
       "  'end': 18},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.9796019,\n",
       "  'word': 'Hugging Face',\n",
       "  'start': 33,\n",
       "  'end': 45},\n",
       " {'entity_group': 'LOC',\n",
       "  'score': 0.9932106,\n",
       "  'word': 'Brooklyn',\n",
       "  'start': 49,\n",
       "  'end': 57}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner = pipeline(\"ner\", grouped_entities=True) # pipeline de reconhecimento de entidades nomeadas e agrupa tokens consecutivos da mesma entidade\n",
    "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7d7ffa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 20:57:44.290701: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "/home/polia/Venvs/ner/lib/python3.12/site-packages/torch/nn/modules/module.py:2397: UserWarning: for bert.embeddings.word_embeddings.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BertForTokenClassification:\n\tsize mismatch for bert.embeddings.position_embeddings.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([512, 768]).\n\tsize mismatch for bert.embeddings.token_type_embeddings.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([2, 768]).\n\tsize mismatch for bert.embeddings.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.embeddings.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.0.attention.self.query.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.0.attention.self.query.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.0.attention.self.key.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.0.attention.self.key.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.0.attention.self.value.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.0.attention.self.value.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.0.attention.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.0.attention.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.0.attention.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.0.attention.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.0.intermediate.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for bert.encoder.layer.0.intermediate.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for bert.encoder.layer.0.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for bert.encoder.layer.0.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.0.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.0.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.1.attention.self.query.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.1.attention.self.query.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.1.attention.self.key.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.1.attention.self.key.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.1.attention.self.value.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.1.attention.self.value.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.1.attention.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.1.attention.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.1.attention.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.1.attention.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.1.intermediate.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for bert.encoder.layer.1.intermediate.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for bert.encoder.layer.1.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for bert.encoder.layer.1.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.1.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.1.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.2.attention.self.query.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.2.attention.self.query.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.2.attention.self.key.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.2.attention.self.key.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.2.attention.self.value.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.2.attention.self.value.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.2.attention.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.2.attention.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.2.attention.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.2.attention.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.2.intermediate.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for bert.encoder.layer.2.intermediate.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for bert.encoder.layer.2.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for bert.encoder.layer.2.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.2.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.2.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.3.attention.self.query.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.3.attention.self.query.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.3.attention.self.key.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.3.attention.self.key.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.3.attention.self.value.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.3.attention.self.value.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.3.attention.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.3.attention.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.3.attention.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.3.attention.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.3.intermediate.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for bert.encoder.layer.3.intermediate.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for bert.encoder.layer.3.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for bert.encoder.layer.3.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.3.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.3.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.4.attention.self.query.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.4.attention.self.query.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.4.attention.self.key.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.4.attention.self.key.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.4.attention.self.value.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.4.attention.self.value.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.4.attention.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.4.attention.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.4.attention.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.4.attention.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.4.intermediate.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for bert.encoder.layer.4.intermediate.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for bert.encoder.layer.4.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for bert.encoder.layer.4.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.4.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.4.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.5.attention.self.query.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.5.attention.self.query.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.5.attention.self.key.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.5.attention.self.key.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.5.attention.self.value.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.5.attention.self.value.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.5.attention.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.5.attention.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.5.attention.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.5.attention.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.5.intermediate.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for bert.encoder.layer.5.intermediate.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for bert.encoder.layer.5.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for bert.encoder.layer.5.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.5.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.5.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.6.attention.self.query.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.6.attention.self.query.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.6.attention.self.key.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.6.attention.self.key.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.6.attention.self.value.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.6.attention.self.value.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.6.attention.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.6.attention.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.6.attention.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.6.attention.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.6.intermediate.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for bert.encoder.layer.6.intermediate.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for bert.encoder.layer.6.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for bert.encoder.layer.6.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.6.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.6.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.7.attention.self.query.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.7.attention.self.query.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.7.attention.self.key.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.7.attention.self.key.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.7.attention.self.value.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.7.attention.self.value.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.7.attention.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.7.attention.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.7.attention.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.7.attention.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.7.intermediate.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for bert.encoder.layer.7.intermediate.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for bert.encoder.layer.7.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for bert.encoder.layer.7.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.7.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.7.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.8.attention.self.query.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.8.attention.self.query.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.8.attention.self.key.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.8.attention.self.key.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.8.attention.self.value.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.8.attention.self.value.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.8.attention.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.8.attention.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.8.attention.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.8.attention.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.8.intermediate.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for bert.encoder.layer.8.intermediate.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for bert.encoder.layer.8.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for bert.encoder.layer.8.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.8.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.8.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.9.attention.self.query.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.9.attention.self.query.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.9.attention.self.key.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.9.attention.self.key.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.9.attention.self.value.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.9.attention.self.value.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.9.attention.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.9.attention.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.9.attention.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.9.attention.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.9.intermediate.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for bert.encoder.layer.9.intermediate.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for bert.encoder.layer.9.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for bert.encoder.layer.9.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.9.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.9.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.10.attention.self.query.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.10.attention.self.query.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.10.attention.self.key.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.10.attention.self.key.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.10.attention.self.value.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.10.attention.self.value.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.10.attention.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.10.attention.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.10.attention.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.10.attention.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.10.intermediate.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for bert.encoder.layer.10.intermediate.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for bert.encoder.layer.10.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for bert.encoder.layer.10.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.10.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.10.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.11.attention.self.query.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.11.attention.self.query.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.11.attention.self.key.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.11.attention.self.key.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.11.attention.self.value.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.11.attention.self.value.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.11.attention.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.11.attention.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.11.attention.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.11.attention.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.11.intermediate.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for bert.encoder.layer.11.intermediate.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for bert.encoder.layer.11.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for bert.encoder.layer.11.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.11.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.11.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for classifier.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([9, 768]).\n\tsize mismatch for classifier.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([9]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mbotryan96/GeoBERT_NER\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m model = \u001b[43mAutoModelForTokenClassification\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adicione este parâmetro\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Criando o pipeline de NER\u001b[39;00m\n\u001b[32m     10\u001b[39m ner_pipeline = pipeline(\u001b[33m\"\u001b[39m\u001b[33mner\u001b[39m\u001b[33m\"\u001b[39m, model=model, tokenizer=tokenizer, aggregation_strategy=\u001b[33m\"\u001b[39m\u001b[33msimple\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Venvs/ner/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:571\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    570\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    575\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    577\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Venvs/ner/lib/python3.12/site-packages/transformers/modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Venvs/ner/lib/python3.12/site-packages/transformers/modeling_utils.py:4384\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4382\u001b[39m \u001b[38;5;66;03m# Finalize model weight initialization\u001b[39;00m\n\u001b[32m   4383\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m from_tf:\n\u001b[32m-> \u001b[39m\u001b[32m4384\u001b[39m     model, loading_info = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_from_tf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4385\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m from_flax:\n\u001b[32m   4386\u001b[39m     model = \u001b[38;5;28mcls\u001b[39m._load_from_flax(model, checkpoint_files)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Venvs/ner/lib/python3.12/site-packages/transformers/modeling_utils.py:4971\u001b[39m, in \u001b[36mPreTrainedModel._load_from_tf\u001b[39m\u001b[34m(cls, model, config, checkpoint_files)\u001b[39m\n\u001b[32m   4968\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   4969\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_tf_pytorch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_tf2_checkpoint_in_pytorch_model\n\u001b[32m-> \u001b[39m\u001b[32m4971\u001b[39m     model, loading_info = \u001b[43mload_tf2_checkpoint_in_pytorch_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4972\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_missing_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_loading_info\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m   4973\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4974\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m   4975\u001b[39m     logger.error(\n\u001b[32m   4976\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mLoading a TensorFlow model in PyTorch, requires both PyTorch and TensorFlow to be installed.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4977\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m Please see https://pytorch.org/ and https://www.tensorflow.org/install/ for installation\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4978\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m instructions.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4979\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Venvs/ner/lib/python3.12/site-packages/transformers/modeling_tf_pytorch_utils.py:530\u001b[39m, in \u001b[36mload_tf2_checkpoint_in_pytorch_model\u001b[39m\u001b[34m(pt_model, tf_checkpoint_path, tf_inputs, allow_missing_keys, output_loading_info)\u001b[39m\n\u001b[32m    526\u001b[39m     tf_model(tf_inputs, training=\u001b[38;5;28;01mFalse\u001b[39;00m)  \u001b[38;5;66;03m# Make sure model is built\u001b[39;00m\n\u001b[32m    528\u001b[39m load_tf_weights(tf_model, tf_checkpoint_path)\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_tf2_model_in_pytorch_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    531\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpt_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_missing_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_missing_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_loading_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_loading_info\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Venvs/ner/lib/python3.12/site-packages/transformers/modeling_tf_pytorch_utils.py:539\u001b[39m, in \u001b[36mload_tf2_model_in_pytorch_model\u001b[39m\u001b[34m(pt_model, tf_model, allow_missing_keys, output_loading_info)\u001b[39m\n\u001b[32m    536\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Load TF 2.0 model in a pytorch model\"\"\"\u001b[39;00m\n\u001b[32m    537\u001b[39m weights = tf_model.weights\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_tf2_weights_in_pytorch_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpt_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_missing_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_missing_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_loading_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_loading_info\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Venvs/ner/lib/python3.12/site-packages/transformers/modeling_tf_pytorch_utils.py:557\u001b[39m, in \u001b[36mload_tf2_weights_in_pytorch_model\u001b[39m\u001b[34m(pt_model, tf_weights, allow_missing_keys, output_loading_info)\u001b[39m\n\u001b[32m    554\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    556\u001b[39m tf_state_dict = {tf_weight.name: tf_weight.numpy() \u001b[38;5;28;01mfor\u001b[39;00m tf_weight \u001b[38;5;129;01min\u001b[39;00m tf_weights}\n\u001b[32m--> \u001b[39m\u001b[32m557\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_tf2_state_dict_in_pytorch_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpt_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_missing_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_missing_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_loading_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_loading_info\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Venvs/ner/lib/python3.12/site-packages/transformers/modeling_tf_pytorch_utils.py:627\u001b[39m, in \u001b[36mload_tf2_state_dict_in_pytorch_model\u001b[39m\u001b[34m(pt_model, tf_state_dict, allow_missing_keys, output_loading_info)\u001b[39m\n\u001b[32m    624\u001b[39m     loaded_pt_weights_data_ptr[pt_weight.data_ptr()] = array\n\u001b[32m    625\u001b[39m     all_tf_weights.discard(pt_weight_name)\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m missing_keys, unexpected_keys = \u001b[43mpt_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_pt_params_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    628\u001b[39m missing_keys += missing_keys_pt\n\u001b[32m    630\u001b[39m \u001b[38;5;66;03m# Some models may have keys that are not in the state by design, removing them before needlessly warning\u001b[39;00m\n\u001b[32m    631\u001b[39m \u001b[38;5;66;03m# the user.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Venvs/ner/lib/python3.12/site-packages/torch/nn/modules/module.py:2581\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2573\u001b[39m         error_msgs.insert(\n\u001b[32m   2574\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2575\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2576\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2577\u001b[39m             ),\n\u001b[32m   2578\u001b[39m         )\n\u001b[32m   2580\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2581\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2583\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2584\u001b[39m         )\n\u001b[32m   2585\u001b[39m     )\n\u001b[32m   2586\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for BertForTokenClassification:\n\tsize mismatch for bert.embeddings.position_embeddings.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([512, 768]).\n\tsize mismatch for bert.embeddings.token_type_embeddings.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([2, 768]).\n\tsize mismatch for bert.embeddings.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.embeddings.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.0.attention.self.query.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.0.attention.self.query.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.0.attention.self.key.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.0.attention.self.key.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.0.attention.self.value.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.0.attention.self.value.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.0.attention.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.0.attention.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.0.attention.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.0.attention.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.0.intermediate.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for bert.encoder.layer.0.intermediate.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for bert.encoder.layer.0.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for bert.encoder.layer.0.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.0.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.0.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.1.attention.self.query.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.1.attention.self.query.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.1.attention.self.key.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.1.attention.self.key.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.1.attention.self.value.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.1.attention.self.value.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.1.attention.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.1.attention.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.1.attention.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.1.attention.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.1.intermediate.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for bert.encoder.layer.1.intermediate.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for bert.encoder.layer.1.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for bert.encoder.layer.1.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.1.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.1.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.2.attention.self.query.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.2.attention.self.query.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.2.attention.self.key.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.2.attention.self.key.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.2.attention.self.value.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.2.attention.self.value.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.2.attention.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.2.attention.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.2.attention.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.2.attention.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.2.intermediate.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for bert.encoder.layer.2.intermediate.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for bert.encoder.layer.2.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for bert.encoder.layer.2.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.2.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.2.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.3.attention.self.query.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.3.attention.self.query.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.3.attention.self.key.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.3.attention.self.key.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.3.attention.self.value.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.3.attention.self.value.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.3.attention.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.3.attention.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.3.attention.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.3.attention.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.3.intermediate.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for bert.encoder.layer.3.intermediate.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for bert.encoder.layer.3.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for bert.encoder.layer.3.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.3.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.3.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.4.attention.self.query.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.4.attention.self.query.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.4.attention.self.key.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.4.attention.self.key.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.4.attention.self.value.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.4.attention.self.value.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.4.attention.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.4.attention.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.4.attention.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.4.attention.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.4.intermediate.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for bert.encoder.layer.4.intermediate.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for bert.encoder.layer.4.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for bert.encoder.layer.4.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.4.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.4.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.5.attention.self.query.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.5.attention.self.query.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.5.attention.self.key.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.5.attention.self.key.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.5.attention.self.value.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.5.attention.self.value.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.5.attention.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.5.attention.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.5.attention.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.5.attention.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.5.intermediate.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for bert.encoder.layer.5.intermediate.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for bert.encoder.layer.5.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for bert.encoder.layer.5.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.5.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.5.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.6.attention.self.query.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.6.attention.self.query.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.6.attention.self.key.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.6.attention.self.key.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.6.attention.self.value.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.6.attention.self.value.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.6.attention.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.6.attention.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.6.attention.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.6.attention.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.6.intermediate.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for bert.encoder.layer.6.intermediate.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for bert.encoder.layer.6.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for bert.encoder.layer.6.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.6.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.6.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.7.attention.self.query.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.7.attention.self.query.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.7.attention.self.key.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.7.attention.self.key.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.7.attention.self.value.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.7.attention.self.value.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.7.attention.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.7.attention.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.7.attention.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.7.attention.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.7.intermediate.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for bert.encoder.layer.7.intermediate.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for bert.encoder.layer.7.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for bert.encoder.layer.7.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.7.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.7.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.8.attention.self.query.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.8.attention.self.query.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.8.attention.self.key.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.8.attention.self.key.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.8.attention.self.value.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.8.attention.self.value.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.8.attention.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.8.attention.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.8.attention.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.8.attention.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.8.intermediate.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for bert.encoder.layer.8.intermediate.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for bert.encoder.layer.8.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for bert.encoder.layer.8.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.8.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.8.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.9.attention.self.query.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.9.attention.self.query.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.9.attention.self.key.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.9.attention.self.key.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.9.attention.self.value.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.9.attention.self.value.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.9.attention.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.9.attention.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.9.attention.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.9.attention.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.9.intermediate.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for bert.encoder.layer.9.intermediate.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for bert.encoder.layer.9.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for bert.encoder.layer.9.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.9.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.9.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.10.attention.self.query.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.10.attention.self.query.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.10.attention.self.key.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.10.attention.self.key.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.10.attention.self.value.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.10.attention.self.value.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.10.attention.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.10.attention.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.10.attention.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.10.attention.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.10.intermediate.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for bert.encoder.layer.10.intermediate.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for bert.encoder.layer.10.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for bert.encoder.layer.10.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.10.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.10.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.11.attention.self.query.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.11.attention.self.query.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.11.attention.self.key.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.11.attention.self.key.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.11.attention.self.value.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.11.attention.self.value.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.11.attention.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for bert.encoder.layer.11.attention.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.11.attention.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.11.attention.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.11.intermediate.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072, 768]).\n\tsize mismatch for bert.encoder.layer.11.intermediate.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([3072]).\n\tsize mismatch for bert.encoder.layer.11.output.dense.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768, 3072]).\n\tsize mismatch for bert.encoder.layer.11.output.dense.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.11.output.LayerNorm.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for bert.encoder.layer.11.output.LayerNorm.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for classifier.weight: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([9, 768]).\n\tsize mismatch for classifier.bias: copying a param with shape torch.Size([31090, 768]) from checkpoint, the shape in current model is torch.Size([9])."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Carregando o tokenizer e o modelo com from_tf=True\n",
    "model_name = \"botryan96/GeoBERT_NER\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, from_tf=True)  # Adicione este parâmetro\n",
    "\n",
    "# Criando o pipeline de NER\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Exemplo de texto para testar\n",
    "text = \"The Cambrian period saw significant sediment deposition in the Appalachian Basin using seismic methods.\"\n",
    "\n",
    "# Fazendo a predição\n",
    "entities = ner_pipeline(text)\n",
    "\n",
    "# Exibindo as entidades\n",
    "for entity in entities:\n",
    "    print(entity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
