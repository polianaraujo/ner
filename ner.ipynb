{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1335ca34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Santos → BACIA (0.61)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Carregar modelo e tokenizer pré-treinados para NER\n",
    "model_name = \"vabatista/geological-ner\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Criar pipeline de NER\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Texto de exemplo\n",
    "text = \"The Santos Basin contains sandstone reservoirs and salt diapirs.\"\n",
    "\n",
    "# Rodar o NER\n",
    "entities = ner_pipeline(text)\n",
    "\n",
    "# Mostrar entidades encontradas\n",
    "for entity in entities:\n",
    "    print(f\"{entity['word']} → {entity['entity_group']} ({entity['score']:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98e60e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bacia de Santos → BACIA (1.00)\n",
      "are → ROCHA (1.00)\n",
      "##ni → ROCHA (0.98)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Carregar modelo e tokenizer pré-treinados para NER\n",
    "model_name = \"vabatista/geological-ner\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Criar pipeline de NER\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Texto de exemplo\n",
    "# text = \"The Santos Basin contains sandstone reservoirs and salt diapirs.\"\n",
    "text = \"A Bacia de Santos contem reservatorios de arenito e diapiros de sal.\"\n",
    "\n",
    "# Rodar o NER\n",
    "entities = ner_pipeline(text)\n",
    "\n",
    "# Mostrar entidades encontradas\n",
    "for entity in entities:\n",
    "    print(f\"{entity['word']} → {entity['entity_group']} ({entity['score']:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c764bd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estrutura → ESTRUTURA_FÍSICA (1.00)\n",
      "triste → ESTRUTURA_FÍSICA (0.59)\n",
      "estrutura → ESTRUTURA_FÍSICA (0.99)\n",
      "estrutura → ESTRUTURA_FÍSICA (0.99)\n",
      "estruturas → ESTRUTURA_FÍSICA (0.99)\n",
      "estruturas → ESTRUTURA_FÍSICA (0.99)\n",
      "mag → ROCHA (0.94)\n",
      "estruturas → ESTRUTURA_FÍSICA (0.99)\n",
      "fra → ESTRUTURA_FÍSICA (0.99)\n",
      "fra → ESTRUTURA_FÍSICA (0.99)\n",
      "fra → ESTRUTURA_FÍSICA (0.97)\n",
      "fra → ESTRUTURA_FÍSICA (0.99)\n",
      "fra → ESTRUTURA_FÍSICA (0.99)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "# Carregar modelo e tokenizer pré-treinados para NER\n",
    "model_name = \"vabatista/geological-ner\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Criar pipeline de NER\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Texto de exemplo\n",
    "# text = \"The Santos Basin contains sandstone reservoirs and salt diapirs.\"\n",
    "with open(\"whisper-audio-imagem1.txt\", \"r\", encoding=\"utf-8\") as arquivo:\n",
    "    text = arquivo.read()\n",
    "\n",
    "# Rodar o NER\n",
    "entities = ner_pipeline(text)\n",
    "\n",
    "# Mostrar entidades encontradas\n",
    "for entity in entities:\n",
    "    print(f\"{entity['word']} → {entity['entity_group']} ({entity['score']:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "503e13a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Essa imagem mostra a porção central da elevação do Rio Grande. Uma estrutura que fica localizada no Atlântico Sul é na sua porção ao este e está geologicamente relacionada à abertura do seu Atlântico Sul e teria a sua principal hipótese de origem a pluma, tristeão da cunha e gos. Em essa estrutura ela teria na porção ao este, uma a elevação do Rio Grande e uma estrutura correlata na porção leste que acadeia valos. Essas duas estruturas são cadeias de montanha, montanha e submarinas que estão relacionadas a este hotspot, essa pluma manteiga possivelmente. E as duas estruturas seriam correlatas, estariam relacionadas com a movimentação das placas tectônicas a medida que o océano foi abrindo. E esse hotspot, esse ponto fixo do manto que emitia esse magma, ele teria formado essas estruturas. Essa parte é mostrando a localização da elevação do Rio Grande na Atlântico Sul e a parte B é um zoom mostrando a porção central dessa montanha submarina a elevação do Rio Grande e também as figuras mostram zonas de fratura que afeta essa região. As três principais são as zonas de fratura, Rio Grande, a zona de fratura, Porto Alegre e a zona de fratura Meteol. A elevação do Rio Grande também foi afetada por um rípti muito expressivo que foi o rípti cruzeiro do sul que está sendo mostrado também nessa imagem. Na parte B a gente vê o zoom e eu estou mostrando a movimentação tectônica da zona de fratura Porto Alegre que teria afetado fortemente o rípti cruzeiro do sul e essa porção que será discutida. São quatro linhas círdicas que foram utilizadas. Essas linhas círdicas foram finca de reflexão e conflindo também com o furo de sondagem o 1516F. O furo foi obtido no DPSIA Drilling Project o DSDP. As linhas foram betidas no Instituto de Jofísca da Universidade de Texas, no Banco de Dades Público. Foi feito nessas linhas, um processamento, uma amigração, um software promise e interpretação sígena que foi feito no software Open-E-Tech e o modelo evolutivo geológico no código. Foi feita uma migração dessas linhas círdicas que são antigas para se obter novas informações. Foi feita com relação com as informações do furo de sondagem e a interpretação sígena stratigrapha e a conflição de um modelo evolutivo com evidências tectônicas e bucança alimentares.\n"
     ]
    }
   ],
   "source": [
    "with open(\"whisper-audio-imagem1.txt\", \"r\", encoding=\"utf-8\") as arquivo:\n",
    "    texto = arquivo.read()\n",
    "    \n",
    "print(texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89186e9",
   "metadata": {},
   "source": [
    "## Petrolês"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "971794b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeDecodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m caminho_modelo = \u001b[33m'\u001b[39m\u001b[33mpublico-COMPLETO-100.txt.model\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Carregar o modelo Word2Vec/FastText\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m modelo = \u001b[43mKeyedVectors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcaminho_modelo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Testar: buscar palavras similares a 'reservatório'\u001b[39;00m\n\u001b[32m     10\u001b[39m similaridade = modelo.most_similar(\u001b[33m'\u001b[39m\u001b[33mreservatório\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Venvs/ner/lib/python3.12/site-packages/gensim/models/keyedvectors.py:1719\u001b[39m, in \u001b[36mKeyedVectors.load_word2vec_format\u001b[39m\u001b[34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[39m\n\u001b[32m   1672\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1673\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_word2vec_format\u001b[39m(\n\u001b[32m   1674\u001b[39m         \u001b[38;5;28mcls\u001b[39m, fname, fvocab=\u001b[38;5;28;01mNone\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf8\u001b[39m\u001b[33m'\u001b[39m, unicode_errors=\u001b[33m'\u001b[39m\u001b[33mstrict\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   1675\u001b[39m         limit=\u001b[38;5;28;01mNone\u001b[39;00m, datatype=REAL, no_header=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1676\u001b[39m     ):\n\u001b[32m   1677\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[32m   1678\u001b[39m \n\u001b[32m   1679\u001b[39m \u001b[33;03m    Warnings\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1717\u001b[39m \n\u001b[32m   1718\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_word2vec_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1720\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfvocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43municode_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[43municode_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1721\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatatype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdatatype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_header\u001b[49m\u001b[43m=\u001b[49m\u001b[43mno_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1722\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Venvs/ner/lib/python3.12/site-packages/gensim/models/keyedvectors.py:2058\u001b[39m, in \u001b[36m_load_word2vec_format\u001b[39m\u001b[34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[39m\n\u001b[32m   2056\u001b[39m     fin = utils.open(fname, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   2057\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2058\u001b[39m     header = \u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_unicode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2059\u001b[39m     vocab_size, vector_size = [\u001b[38;5;28mint\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m header.split()]  \u001b[38;5;66;03m# throws for invalid file format\u001b[39;00m\n\u001b[32m   2060\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m limit:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Venvs/ner/lib/python3.12/site-packages/gensim/utils.py:364\u001b[39m, in \u001b[36many2unicode\u001b[39m\u001b[34m(text, encoding, errors)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mUnicodeDecodeError\u001b[39m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Caminho para o arquivo\n",
    "caminho_modelo = 'publico-COMPLETO-100.txt.model'\n",
    "\n",
    "# Carregar o modelo Word2Vec/FastText\n",
    "modelo = KeyedVectors.load_word2vec_format(caminho_modelo, binary=True)\n",
    "\n",
    "# Testar: buscar palavras similares a 'reservatório'\n",
    "similaridade = modelo.most_similar('reservatório')\n",
    "\n",
    "print(similaridade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df58be1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x80\\x02cgensim.models.word2vec\\nWord2Vec\\nq\\x00)\\x81q\\x01}q\\x02(X\\x0f\\x00\\x00\\x00max_final_vocabq\\x03NX\\t\\x00\\x00\\x00callbacksq\\x04)X\\x04\\x00\\x00\\x00loadq\\x05cgen'\n"
     ]
    }
   ],
   "source": [
    "with open('publico-COMPLETO-100.txt.model', 'rb') as f:\n",
    "    cabecalho = f.read(100)\n",
    "    print(cabecalho)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5791a091",
   "metadata": {},
   "source": [
    "um objeto Word2Vec serializado via Pickle. Portanto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f198b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('subbacia', 0.7549439668655396), ('bacias', 0.7437440156936646), ('baaa', 0.7033339142799377), ('sacia', 0.6683410406112671), ('sbse', 0.6528235673904419), ('provincia', 0.6494128704071045), ('canion', 0.6358173489570618), ('sedimentaqiio', 0.6222701668739319), ('baiano', 0.6168858408927917), ('regiiio', 0.6151022911071777)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Carregar o modelo salvo em formato .model (Pickle)\n",
    "modelo = Word2Vec.load('publico-COMPLETO-100.txt.model')\n",
    "\n",
    "# Consultar similaridade — acessando o KeyedVectors do modelo\n",
    "similaridade = modelo.wv.most_similar('bacia')\n",
    "print(similaridade)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
