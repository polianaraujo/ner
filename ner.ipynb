{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95521d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed2979",
   "metadata": {},
   "source": [
    "#### Teste de modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98e60e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bacia de Santos → BACIA (1.00)\n",
      "are → ROCHA (1.00)\n",
      "##ni → ROCHA (0.98)\n"
     ]
    }
   ],
   "source": [
    "# Carregar modelo e tokenizer pré-treinados para NER\n",
    "model_name = \"vabatista/geological-ner\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Criar pipeline de NER\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Texto de exemplo\n",
    "# text = \"The Santos Basin contains sandstone reservoirs and salt diapirs.\"\n",
    "text = \"A Bacia de Santos contem reservatorios de arenito e diapiros de sal.\"\n",
    "\n",
    "# Rodar o NER\n",
    "entities = ner_pipeline(text)\n",
    "\n",
    "# Mostrar entidades encontradas\n",
    "for entity in entities:\n",
    "    print(f\"{entity['word']} → {entity['entity_group']} ({entity['score']:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1148a09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bacia de Campos → BACIA (1.00)\n",
      "petróleo → FLUIDODATERRA_i (0.99)\n",
      "carb → ROCHA (0.99)\n",
      "##onato → ROCHA (0.94)\n",
      "bas → ROCHA (1.00)\n",
      "##alto → ROCHA (0.98)\n",
      "falhas → ESTRUTURA_FÍSICA (1.00)\n",
      "Bacia do Espírito Santo → BACIA (1.00)\n",
      "folhe → ROCHA (1.00)\n",
      "##lho → ROCHA (0.84)\n"
     ]
    }
   ],
   "source": [
    "# Carregar modelo e tokenizer pré-treinados para NER\n",
    "model_name = \"vabatista/geological-ner\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Criar pipeline de NER\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "# Texto de exemplo\n",
    "text = \"A Bacia de Campos é conhecida por sua produção de petróleo e extensos reservatórios de carbonato. Na região, fluxos de basalto recobrem sequências sedimentares, e falhas são comuns devido à atividade tectônica. Próxima dali, a Bacia do Espírito Santo também apresenta sistemas de turbiditos e formações de folhelho. Pesquisadores também identificaram corpos intrusivos e diapiros de sal nessas áreas.\"\n",
    "\n",
    "# Rodar o NER\n",
    "entities = ner_pipeline(text)\n",
    "\n",
    "# Mostrar entidades encontradas\n",
    "for entity in entities:\n",
    "    print(f\"{entity['word']} → {entity['entity_group']} ({entity['score']:.2f})\")\n",
    "    \n",
    "# letras : t, s, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae8de9c",
   "metadata": {},
   "source": [
    "#### Teste de transcrição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8eb8fbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "# Carregar modelo e tokenizer\n",
    "model_name = \"vabatista/geological-ner\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Pipeline\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "503e13a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Essa imagem mostra a porção central da elevação do Rio Grande. Uma estrutura que fica localizada no Atlântico Sul é na sua porção ao este e está geologicamente relacionada à abertura do seu Atlântico Sul e teria a sua principal hipótese de origem a pluma, tristeão da cunha e gos. Em essa estrutura ela teria na porção ao este, uma a elevação do Rio Grande e uma estrutura correlata na porção leste que acadeia valos. Essas duas estruturas são cadeias de montanha, montanha e submarinas que estão relacionadas a este hotspot, essa pluma manteiga possivelmente. E as duas estruturas seriam correlatas, estariam relacionadas com a movimentação das placas tectônicas a medida que o océano foi abrindo. E esse hotspot, esse ponto fixo do manto que emitia esse magma, ele teria formado essas estruturas. Essa parte é mostrando a localização da elevação do Rio Grande na Atlântico Sul e a parte B é um zoom mostrando a porção central dessa montanha submarina a elevação do Rio Grande e também as figuras mostram zonas de fratura que afeta essa região. As três principais são as zonas de fratura, Rio Grande, a zona de fratura, Porto Alegre e a zona de fratura Meteol. A elevação do Rio Grande também foi afetada por um rípti muito expressivo que foi o rípti cruzeiro do sul que está sendo mostrado também nessa imagem. Na parte B a gente vê o zoom e eu estou mostrando a movimentação tectônica da zona de fratura Porto Alegre que teria afetado fortemente o rípti cruzeiro do sul e essa porção que será discutida. São quatro linhas círdicas que foram utilizadas. Essas linhas círdicas foram finca de reflexão e conflindo também com o furo de sondagem o 1516F. O furo foi obtido no DPSIA Drilling Project o DSDP. As linhas foram betidas no Instituto de Jofísca da Universidade de Texas, no Banco de Dades Público. Foi feito nessas linhas, um processamento, uma amigração, um software promise e interpretação sígena que foi feito no software Open-E-Tech e o modelo evolutivo geológico no código. Foi feita uma migração dessas linhas círdicas que são antigas para se obter novas informações. Foi feita com relação com as informações do furo de sondagem e a interpretação sígena stratigrapha e a conflição de um modelo evolutivo com evidências tectônicas e bucança alimentares.\n"
     ]
    }
   ],
   "source": [
    "with open(\"./transcriptions/whisper-audio-imagem1.txt\", \"r\", encoding=\"utf-8\") as arquivo:\n",
    "    texto = arquivo.read()\n",
    "    \n",
    "print(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c764bd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estrutura → ESTRUTURA_FÍSICA (1.00)\n",
      "triste → ESTRUTURA_FÍSICA (0.59)\n",
      "estrutura → ESTRUTURA_FÍSICA (0.99)\n",
      "estrutura → ESTRUTURA_FÍSICA (0.99)\n",
      "estruturas → ESTRUTURA_FÍSICA (0.99)\n",
      "estruturas → ESTRUTURA_FÍSICA (0.99)\n",
      "mag → ROCHA (0.94)\n",
      "estruturas → ESTRUTURA_FÍSICA (0.99)\n",
      "fra → ESTRUTURA_FÍSICA (0.99)\n",
      "fra → ESTRUTURA_FÍSICA (0.99)\n",
      "fra → ESTRUTURA_FÍSICA (0.97)\n",
      "fra → ESTRUTURA_FÍSICA (0.99)\n",
      "fra → ESTRUTURA_FÍSICA (0.99)\n"
     ]
    }
   ],
   "source": [
    "# Texto de exemplo\n",
    "# text = \"The Santos Basin contains sandstone reservoirs and salt diapirs.\"\n",
    "with open(\"./transcriptions/whisper-audio-imagem1.txt\", \"r\", encoding=\"utf-8\") as arquivo:\n",
    "    text = arquivo.read()\n",
    "\n",
    "# Rodar o NER\n",
    "entities = ner_pipeline(text)\n",
    "\n",
    "# Mostrar entidades encontradas\n",
    "for entity in entities:\n",
    "    print(f\"{entity['word']} → {entity['entity_group']} ({entity['score']:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b76793",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estrutura → ESTRUTURA_FÍSICA (0.99)\n",
      "estrutura → ESTRUTURA_FÍSICA (0.99)\n",
      "estrutura → ESTRUTURA_FÍSICA (0.99)\n",
      "estruturas → ESTRUTURA_FÍSICA (0.99)\n",
      "estruturas → ESTRUTURA_FÍSICA (0.99)\n",
      "mag → ROCHA (0.97)\n",
      "estruturas → ESTRUTURA_FÍSICA (0.99)\n",
      "fra → ESTRUTURA_FÍSICA (0.99)\n",
      "##tura → ESTRUTURA_FÍSICA (0.49)\n",
      "fra → ESTRUTURA_FÍSICA (0.98)\n",
      "fra → ESTRUTURA_FÍSICA (0.99)\n",
      "fra → ESTRUTURA_FÍSICA (0.98)\n",
      "fra → ESTRUTURA_FÍSICA (0.99)\n"
     ]
    }
   ],
   "source": [
    "# Carregar texto\n",
    "with open(\"./transcriptions/gemini-audio-imagem1.txt\", \"r\", encoding=\"utf-8\") as arquivo:\n",
    "    text = arquivo.read()\n",
    "\n",
    "# Dividir em chunks de 512 tokens\n",
    "max_length = 512\n",
    "tokens = tokenizer(text, return_tensors=\"pt\", truncation=False)[\"input_ids\"][0]\n",
    "chunks = tokens.split(max_length - 2)  # espaço para [CLS] e [SEP]\n",
    "\n",
    "# Processar cada chunk\n",
    "entities = []\n",
    "for chunk in chunks:\n",
    "    # Reconstrói o texto desse pedaço\n",
    "    chunk_text = tokenizer.decode(chunk, skip_special_tokens=True)\n",
    "    chunk_entities = ner_pipeline(chunk_text)\n",
    "    entities.extend(chunk_entities)\n",
    "\n",
    "# Mostrar\n",
    "for entity in entities:\n",
    "    print(f\"{entity['word']} → {entity['entity_group']} ({entity['score']:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b21b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "migração → EVENTO_PETRO (0.94)\n",
      "mo → ESTRUTURA_FÍSICA (0.96)\n",
      "mo → ESTRUTURA_FÍSICA (0.92)\n",
      "falhas → ESTRUTURA_FÍSICA (1.00)\n",
      "estruturas → ESTRUTURA_FÍSICA (1.00)\n",
      "E → UNIDADE_CRONO (0.99)\n",
      "##oceno → UNIDADE_CRONO (0.94)\n",
      "em → UNIDADE_LITO (0.99)\n",
      "##bas → UNIDADE_LITO (0.56)\n",
      "em → UNIDADE_LITO (1.00)\n",
      "##bas → UNIDADE_LITO (0.95)\n",
      "estrutura → ESTRUTURA_FÍSICA (0.99)\n",
      "em → UNIDADE_LITO (1.00)\n",
      "##bas → UNIDADE_LITO (0.85)\n",
      "estruturas → ESTRUTURA_FÍSICA (0.99)\n"
     ]
    }
   ],
   "source": [
    "# Carregar texto\n",
    "with open(\"./transcriptions/0005_transcription.gemini-1.5-flash.txt\", \"r\", encoding=\"utf-8\") as arquivo:\n",
    "    text = arquivo.read()\n",
    "\n",
    "# Dividir em chunks de 512 tokens\n",
    "max_length = 512\n",
    "tokens = tokenizer(text, return_tensors=\"pt\", truncation=False)[\"input_ids\"][0]\n",
    "chunks = tokens.split(max_length - 2)  # espaço para [CLS] e [SEP]\n",
    "\n",
    "# Processar cada chunk\n",
    "entities = []\n",
    "for chunk in chunks:\n",
    "    # Reconstrói o texto desse pedaço\n",
    "    chunk_text = tokenizer.decode(chunk, skip_special_tokens=True)\n",
    "    chunk_entities = ner_pipeline(chunk_text)\n",
    "    entities.extend(chunk_entities)\n",
    "\n",
    "# Mostrar\n",
    "for entity in entities:\n",
    "    print(f\"{entity['word']} → {entity['entity_group']} ({entity['score']:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a226d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bas → ROCHA (1.00)\n",
      "##altos → ROCHA (0.66)\n",
      "em → UNIDADE_LITO (1.00)\n",
      "##bas → UNIDADE_LITO (0.83)\n",
      "calc → ROCHA (1.00)\n",
      "##ários → ROCHA (0.97)\n",
      "bre → ROCHA (0.99)\n",
      "##chas vulc → ROCHA (0.79)\n",
      "bas → ROCHA (1.00)\n",
      "##alto → ROCHA (0.98)\n",
      "idade Coniaciano → UNIDADE_CRONO (0.99)\n",
      "calc → ROCHA (1.00)\n",
      "##ários → ROCHA (0.97)\n",
      "calc → ROCHA (1.00)\n",
      "##ários → ROCHA (0.94)\n",
      "arg → ROCHA (0.99)\n",
      "##ili → ROCHA (0.99)\n",
      "Pale → UNIDADE_CRONO (1.00)\n",
      "##oceno superior → UNIDADE_CRONO (0.98)\n",
      "Santo → UNIDADE_CRONO (0.99)\n",
      "##niano → UNIDADE_CRONO (0.93)\n",
      "Con → UNIDADE_CRONO (0.96)\n",
      "##iaciano → UNIDADE_CRONO (0.79)\n",
      "calc → ROCHA (1.00)\n",
      "##ários → ROCHA (0.94)\n",
      "Idade Eoceno médio → UNIDADE_CRONO (0.99)\n",
      "Pale → UNIDADE_CRONO (1.00)\n",
      "##oceno superior → UNIDADE_CRONO (0.89)\n",
      "calc → ROCHA (1.00)\n",
      "##ários → ROCHA (0.87)\n",
      "cinzas → NÃOCONSOLID (0.81)\n",
      "E → UNIDADE_CRONO (0.99)\n",
      "##oceno médio → UNIDADE_CRONO (0.99)\n",
      "Idade Oligoceno inferior → UNIDADE_CRONO (0.99)\n",
      "E → UNIDADE_CRONO (0.99)\n",
      "##oceno inferior → UNIDADE_CRONO (0.88)\n",
      "Mi → UNIDADE_CRONO (0.99)\n",
      "##oceno superior → UNIDADE_CRONO (0.99)\n",
      "Oligoceno inferior → UNIDADE_CRONO (0.96)\n",
      "Mi → UNIDADE_CRONO (0.99)\n",
      "##oceno superior → UNIDADE_CRONO (0.89)\n"
     ]
    }
   ],
   "source": [
    "# Carregar o CSV\n",
    "df = pd.read_csv('./transcriptions/data-1745593712414.csv')\n",
    "\n",
    "# Pegar a terceira coluna\n",
    "coluna_terceira = df.iloc[:, 2]\n",
    "\n",
    "indice = 12  # escolhe a linha (começa do 0)\n",
    "texto = coluna_terceira.iloc[indice]\n",
    "\n",
    "# Garantir que é string\n",
    "if isinstance(texto, str):\n",
    "    # Agora faz o processo normal\n",
    "    max_length = 512\n",
    "    tokens = tokenizer(texto, return_tensors=\"pt\", truncation=False)[\"input_ids\"][0]\n",
    "    chunks = tokens.split(max_length - 2)  # espaço para [CLS] e [SEP]\n",
    "\n",
    "    entities = []\n",
    "    for chunk in chunks:\n",
    "        chunk_text = tokenizer.decode(chunk, skip_special_tokens=True)\n",
    "        chunk_entities = ner_pipeline(chunk_text)\n",
    "        entities.extend(chunk_entities)\n",
    "\n",
    "    # Mostrar resultados\n",
    "    for entity in entities:\n",
    "        print(f\"{entity['word']} → {entity['entity_group']} ({entity['score']:.2f})\")\n",
    "else:\n",
    "    print(\"A linha escolhida não é um texto válido.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad6a641",
   "metadata": {},
   "source": [
    "### Alguns modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683223ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence': 'The Santos Basin contains sandstone reservoirs and salt diapirs.',\n",
       " 'labels': ['geology', 'rock', 'education'],\n",
       " 'scores': [0.8048369288444519, 0.18419402837753296, 0.010969061404466629]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "classifier(\n",
    "    \"The Santos Basin contains sandstone reservoirs and salt diapirs.\",\n",
    "    candidate_labels=[\"education\", \"geology\", \"rock\"], # categorias candidatas para testar as relações\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de93ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The Santos Basin contains sandstone reservoirs for the cultivation of gold and natural gas. The area is home to about 6,000 megadields of natural gas.\\n\\nThe project has raised $500million and has provided a key source of income'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "generator(\"The Santos Basin contains sandstone reservoirs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab066615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Some weights of the model checkpoint at distilbert/distilroberta-base were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.19619767367839813,\n",
       "  'token': 30412,\n",
       "  'token_str': ' mathematical',\n",
       "  'sequence': 'This course will teach you all about mathematical models.'},\n",
       " {'score': 0.04052715748548508,\n",
       "  'token': 38163,\n",
       "  'token_str': ' computational',\n",
       "  'sequence': 'This course will teach you all about computational models.'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unmasker = pipeline(\"fill-mask\") # pipeline que completa lacunas no texto onde aparece o token <mask>\n",
    "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28380ee",
   "metadata": {},
   "source": [
    "- tentar ajustar parâmetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1de3a31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/tmp/ipykernel_19535/3309837655.py:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resumo gerado:\n",
      "O Furo de sondagem o 1516F mostra a localização da elevação do Rio Grande no Atlântico Sul e as áreas de fratura que afetam essa região. A imagem é feita com quatro partes círdicas. Estas são as duas estruturas de montanha submarinas que estão relacionadas à movimentação das placas tectônicas.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Configurações iniciais\n",
    "WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
    "model_name = \"GiordanoB/mT5_multilingual_XLSum-sumarizacao-PTBR\"\n",
    "\n",
    "# Carregar modelo e tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Configurar dispositivo (GPU se disponível)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "def resumir_texto(texto, max_length=200, min_length=75):\n",
    "    \"\"\"\n",
    "    Função para resumir um texto em português usando o modelo mT5\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Pré-processamento do texto\n",
    "        texto_processado = WHITESPACE_HANDLER(texto)\n",
    "        \n",
    "        # Tokenização\n",
    "        input_ids = tokenizer(\n",
    "            [texto_processado],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )[\"input_ids\"].to(device)\n",
    "        \n",
    "        # Geração do resumo\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            min_length=min_length,\n",
    "            no_repeat_ngram_size=2,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        # Decodificação do resumo\n",
    "        summary = tokenizer.decode(\n",
    "            output_ids[0],\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao resumir o texto: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def resumir_arquivo_txt(caminho_arquivo):\n",
    "    \"\"\"\n",
    "    Função para ler um arquivo TXT e gerar um resumo\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(caminho_arquivo, \"r\", encoding=\"utf-8\") as file:\n",
    "            texto = file.read()\n",
    "        \n",
    "        if not texto.strip():\n",
    "            return \"O arquivo está vazio.\"\n",
    "        \n",
    "        # Se o texto for muito longo, dividir em partes\n",
    "        if len(texto.split()) > 1000:\n",
    "            partes = [texto[i:i+1000] for i in range(0, len(texto), 1000)]\n",
    "            resumos = []\n",
    "            for i, parte in enumerate(partes):\n",
    "                resumo = resumir_texto(parte)\n",
    "                if resumo:\n",
    "                    resumos.append(resumo)\n",
    "                    print(f\"Parte {i+1} resumida com sucesso!\")\n",
    "            return \" \".join(resumos)\n",
    "        else:\n",
    "            return resumir_texto(texto)\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        return f\"Erro: Arquivo '{caminho_arquivo}' não encontrado.\"\n",
    "    except Exception as e:\n",
    "        return f\"Ocorreu um erro: {str(e)}\"\n",
    "\n",
    "# Exemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    caminho = \"./transcriptions/whisper-audio-imagem1.txt\"  # Substitua pelo seu caminho\n",
    "    resultado = resumir_arquivo_txt(caminho)\n",
    "    print(\"\\nResumo gerado:\")\n",
    "    print(resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fdfc045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/tmp/ipykernel_19535/1512242165.py:6: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
      "/home/polia/Venvs/ner/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resumo gerado:\n",
      "O Furo de sondagem o 1516F mostra a localização da elevação do Rio Grande no Atlântico Sul e a área de fratura que afeta a região. O furo é o furo da sondagem, o Furo da Sondagem ao 1515F, a sondagem que ocorreu no último sábado e que foi alvo de uma série de imagens feitas pela BBC Brasil.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Configurações iniciais\n",
    "WHITESPACE_HANDLER = lambda k: re.sub('\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
    "model_name = \"GiordanoB/mT5_multilingual_XLSum-sumarizacao-PTBR\"\n",
    "\n",
    "# Carregar modelo e tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Configurar dispositivo (GPU se disponível)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "def resumir_texto(texto, max_length=200, min_length=75):\n",
    "    \"\"\"\n",
    "    Função para resumir um texto em português usando o modelo mT5\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Pré-processamento do texto\n",
    "        texto_processado = WHITESPACE_HANDLER(texto)\n",
    "        \n",
    "        # Tokenização\n",
    "        input_ids = tokenizer(\n",
    "            [texto_processado],\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        )[\"input_ids\"].to(device)\n",
    "        \n",
    "        # Geração do resumo\n",
    "        output_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=200,\n",
    "            min_length=100,\n",
    "            no_repeat_ngram_size=3,\n",
    "            num_beams=4,\n",
    "            temperature=0.1,  # Reduz criatividade para evitar invenções\n",
    "            do_sample=False   # Desativa geração aleatória\n",
    "        )\n",
    "        \n",
    "        # Decodificação do resumo\n",
    "        summary = tokenizer.decode(\n",
    "            output_ids[0],\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True\n",
    "        )\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao resumir o texto: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def resumir_arquivo_txt(caminho_arquivo):\n",
    "    \"\"\"\n",
    "    Função para ler um arquivo TXT e gerar um resumo\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(caminho_arquivo, \"r\", encoding=\"utf-8\") as file:\n",
    "            texto = file.read()\n",
    "        \n",
    "        if not texto.strip():\n",
    "            return \"O arquivo está vazio.\"\n",
    "        \n",
    "        # Se o texto for muito longo, dividir em partes\n",
    "        if len(texto.split()) > 1000:\n",
    "            partes = [texto[i:i+1000] for i in range(0, len(texto), 1000)]\n",
    "            resumos = []\n",
    "            for i, parte in enumerate(partes):\n",
    "                resumo = resumir_texto(parte)\n",
    "                if resumo:\n",
    "                    resumos.append(resumo)\n",
    "                    print(f\"Parte {i+1} resumida com sucesso!\")\n",
    "            return \" \".join(resumos)\n",
    "        else:\n",
    "            return resumir_texto(texto)\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        return f\"Erro: Arquivo '{caminho_arquivo}' não encontrado.\"\n",
    "    except Exception as e:\n",
    "        return f\"Ocorreu um erro: {str(e)}\"\n",
    "\n",
    "# Exemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    caminho = \"./transcriptions/whisper-audio-imagem1.txt\"  # Substitua pelo seu caminho\n",
    "    resultado = resumir_arquivo_txt(caminho)\n",
    "    print(\"\\nResumo gerado:\")\n",
    "    print(resultado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa82a67b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d18f6c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
